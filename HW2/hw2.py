import numpy as np
import random
import time
from Linear_Regression import linear_regression
from Logistic_Regression import logistic_regression

""" OUTPUT
除了#13 #14有找到符合的選項外，其他題算出的錯誤率都略低於題目提供答案
其中以16題linear regrssion差最多，最接近0.078的為0.09

#13, #14
Squar error of linear regression of Error-in  0.2833409665846848
|Error-in - Error-out| of linear regresstion  0.004503999999999987
#15
Error-out of linear regression  0.05325000000000001
Error-out of logistic regression  0.058839999999999996
#16
Error-out of linear regression 0.078484
Error-out of logistic regression  0.05918999999999999
"""
def generate_data(N, s = False):    
    y = int
    data = np.array([[0,0,0,0]])
    for _ in range(N):
        y = np.random.choice([1,-1])
        if s == False:
            if y == 1:
                x = np.random.multivariate_normal([2, 3], [[0.6, 0], [0, 0.6]])
                x = np.insert(x, 0, 1)
                x = np.append(x, y)
                data = np.append(data, [x], axis = 0)
        elif y == 1:
            x = np.random.multivariate_normal([6, 0], [[0.3, 0], [0, 0.1]])
            x = np.insert(x, 0, 1)
            x = np.append(x, y)
            data = np.append(data, [x], axis = 0)
        if y == -1:
            x = np.random.multivariate_normal([0, 4], [[0.4, 0], [0, 0.4]])
            x = np.insert(x, 0, 1)
            x = np.append(x, y)
            data = np.append(data, [x], axis = 0)

    data = np.delete(data, 0, axis= 0)
    return data

# Inorder to generate Error-in, I'm going to predict y 
# with w generated by Linear regresion/Logistic regression        
def compute_binerror(data, w):
    binerror = int
    shape = np.array(data).shape
    predict_y = np.sign(np.dot(data[:, :shape[1]-1], w))
    binerror = predict_y != data[:, shape[1]-1]
    binerror = sum(binerror)
    return binerror/data.shape[0]

def compute_sqrerror(data, w):
    error_in_vector = data[:, 3] - np.dot(data[:, :3], w) 
    error_in = np.linalg.norm(error_in_vector) ** 2
    error_in = error_in / data.shape[0]
    return error_in


def main():
# Trace_ 紀錄每次iteration的資訊
# 題目並沒有要求計算logistic reg.的error-in

# w_lin就是儲存用linear reg訓練出來的w

# 會先初始[[0,0,0]]是因為每次跌帶我都會呼叫
# np.append(Trace_w, [這次跌帶算出來的w], axix = 0)
# 但np.append要求不能對空物件做append
# 當然，[0,0,0]最後會從第一列刪除
    Trace_w_lin = [[0, 0, 0]]
    Trace_w_lg_reg = [[0, 0, 0]]
    Trace_w_lin_ex16 = [[0, 0, 0]]
    Trace_w_lg_reg_ex16 = [[0, 0, 0]]

    Trace_sqrerr_in = []
    Trace_sqrerr_out = []

    Trace_binerr_in_lin = []
    Trace_binerr_in_lin_ex16 = []
    Trace_binerr_out_lin = []
    Trace_binerr_out_lgreg = []
    Trace_binerr_out_lin_ex16 = []
    Trace_binerr_out_lgreg_ex16 = []

    for _ in range(100):
        random.seed(10*time.time())
        train = generate_data(200)
        train_ex16 = np.append(train, generate_data(20, True), axis= 0)
        test = generate_data(5000)

        # ***Function linear_regression(data) 
        # will also return sqr error by the way
        # Contribute w table, which record each w from iteration 0 ~ 100
        # The major goal is compute average w easily after 100 iterations
        # 計算並紀錄squar error 與 w_linear
        w_lin,sqrerr_in = linear_regression(train)
        Trace_sqrerr_in = np.append(Trace_sqrerr_in, sqrerr_in)
        Trace_w_lin = np.append(Trace_w_lin, [w_lin], axis= 0)
        Trace_sqrerr_out =np.append(Trace_sqrerr_out, compute_sqrerror(test, w_lin))

        #紀錄w_logistic_regression
        w_lg_reg = logistic_regression(train, rate = 0.1)
        Trace_w_lg_reg = np.append(Trace_w_lg_reg, [w_lg_reg], axis= 0)
        
        #順便紀錄ex16訓練出的w_lin跟w_logistic_reg
        w_lin_ex16,sqrerr_in_ex16 = linear_regression(train_ex16)
        Trace_w_lin_ex16 = np.append(Trace_w_lin_ex16, [w_lin_ex16], axis= 0)
        w_lg_reg_ex16 = logistic_regression(train_ex16, 0.1)
        Trace_w_lg_reg_ex16 = np.append(Trace_w_lg_reg_ex16, [w_lg_reg_ex16], axis= 0)

        # 利用上面training出的w去計算binary-error-in
        Trace_binerr_in_lin = np.append(Trace_binerr_in_lin, compute_binerror(data = train, w = w_lin))
        Trace_binerr_in_lin_ex16 = np.append(Trace_binerr_in_lin_ex16, compute_binerror(data = train_ex16, w = w_lin_ex16))
        
        # 同樣利用w_lin與w_log_reg去計算binary-error-out
        Trace_binerr_out_lin = np.append(Trace_binerr_out_lin, compute_binerror(data = test, w = w_lin))
        Trace_binerr_out_lgreg = np.append(Trace_binerr_out_lgreg, compute_binerror(data = test, w = w_lg_reg))
        Trace_binerr_out_lin_ex16 = np.append(Trace_binerr_out_lin_ex16, compute_binerror(test, w_lin_ex16))
        Trace_binerr_out_lgreg_ex16 = np.append(Trace_binerr_out_lgreg_ex16, compute_binerror(test, w_lg_reg_ex16))


    w_lin = np.mean(np.delete(Trace_w_lin, 0, axis= 0), axis= 0)
    w_lg_reg = np.mean(np.delete(Trace_w_lg_reg, 0, axis= 0), axis= 0)
    w_lin_ex16 = np.mean(np.delete(Trace_w_lin_ex16, 0, axis= 0), axis= 0)
    w_lg_reg_ex16 = np.mean(np.delete(Trace_w_lg_reg_ex16, 0, axis= 0), axis= 0)



# Exercise 13 , 14
    print("#13, #14")
    print("Squar error of linear regression of Error-in ", np.mean(Trace_sqrerr_in))
    print("|Error-in - Error-out| of linear regresstion ",np.abs(np.mean(Trace_binerr_in_lin) -np.mean(Trace_binerr_out_lin)))
# Exercise 15 
    print("#15")
    #print('Record of binary error-out(A = linear reg.) = ', Trace_err_out_lin)
    #print('Record of binary error-in(B = logistic reg. = ', Trace_err_out_lgreg)
    print('Error-out of linear regression ', np.mean(Trace_binerr_in_lin))
    print('Error-out of logistic regression ', np.mean(Trace_binerr_out_lgreg))

# Exercise 16 
    print("#16")
    #print('Record of binary error-out(A = linear reg.) = ', Trace_err_out_lin_ex16)
    #print('Record of binary error-in(B = logistic reg. = ', Trace_err_out_lgreg_ex16)
    print('Error-out of linear regression', np.mean(Trace_binerr_out_lin_ex16))
    print('Error-out of logistic regression ', np.mean(Trace_binerr_out_lgreg_ex16))


if __name__ == '__main__':
    main()
